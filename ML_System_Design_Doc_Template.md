# ML System Design Doc - [RU]
## Дизайн ML системы - \<Автоматизаци определения ориентации железнодорожных вагонов\> \<MVP or Production System\> \<Номер\>

## 1. Цели и предпосылки

### 1.1. Зачем идем в разработку продукта?
*   **Бизнес-цель (Product Owner):** Интеграция модуля автоматического распознавания ориентации вагонов в существующую систему мониторинга подвижного состава «Транс-Телематики» для повышения уровня автоматизации и исключения ошибок ручного труда диспетчеров.
*   **Почему станет лучше, чем сейчас (Product Owner & Data Scientist):**
    *   *Сейчас:* Ориентация вагона (какая сторона обращена к камере) определяется оператором вручную при просмотре архива или игнорируется, что снижает точность привязки телеметрии.
    *   *Будет:* Система автоматически детектирует ключевые объекты (тормозной цилиндр, ЗР, вентиль) в видеопотоке и присваивает вагону метку «Левая» / «Правая» сторона в режиме, близком к реальному времени.
*   **Что будем считать успехом итерации с точки зрения бизнеса (Product Owner):** Разработанный и протестированный на реальных данных заказчика прототип (MVP), демонстрирующий точность определения стороны (Accuracy) не ниже 90% на валидационной выборке и готовый к интеграции в контур пилотного проекта.

### 1.2. Бизнес-требования и ограничения
*   **Краткое описание БТ (Product Owner):** Разработать модуль компьютерного зрения, который на вход получает кадры с камеры, установленной вдоль ж/д путей, а на выходе отдает идентификатор вагона (при наличии интеграции) и метку ориентации («лево»/«право»), основываясь на наличии/отсутствии специфических референтных объектов (Тормозной цилиндр, Запасной резервуар, Вентиль ручного тормоза).
*   **Бизнес-ограничения (Product Owner):**
    *   Решение должно работать в условиях плохой погоды (снег, дождь, туман) и ограниченной видимости (ночное время/сумерки).
    *   Скорость прохождения вагона мимо камеры может варьироваться (до 60-80 км/ч).
    *   Использование строго тех камер, которые уже установлены в системах заказчика (нет возможности менять ракурс или FPS).
*   **Что мы ожидаем от конкретной итерации (Product Owner):** Получить воспроизводимый код и docker-образ с моделью, который можно развернуть на тестовых мощностях «Транс-Телематики» для прогона через видеопоток с одного полигона.
*   **Описание бизнес-процесса пилота:**
    1.  Видеопоток с камеры поступает на сервер сбора данных.
    2.  Наш ML-сервис подписывается на топик с новыми кадрами (или читает файлы).
    3.  Для каждого вагона (группы кадров) сервис детектирует референтные объекты.
    4.  Принимается решение об ориентации.
    5.  Метка об ориентации сохраняется в БД мониторинга, привязанная к ID вагона.
*   **Что считаем успешным пилотом? Критерии успеха (Product Owner):**
    *   F1-score детекции объектов (тормозной цилиндр, ЗР, вентиль) > 0.85.
    *   Accuracy итогового определения стороны (на основе правил) > 90%.
    *   Обработка одного кадра занимает не более 200 мс (без учета сетевых задержек) на предоставленном железе.

### 1.3. Что входит в скоуп проекта/итерации, что не входит
*   **На закрытие каких БТ подписываемся в данной итерации (Data Scientist):**
    *   Разработка модели детекции референтных объектов (Object Detection).
    *   Создание алгоритма принятия решения об ориентации на основе правил (Rule-based logic).
    *   Подготовка EDA и датасета для обучения.
    *   Контейнеризация решения (Docker) для простоты развертывания.
*   **Что не будет закрыто (Data Scientist):**
    *   Разработка системы детекции вагонов (нумерации) — используем существующую систему заказчика.
    *   Оптимизация под GPU (первый пилот на CPU).
    *   Веб-интерфейс для разметки или просмотра результатов.
    *   Автоматический ретрейнинг моделей (переобучение).
*   **Описание результата с точки зрения качества кода и воспроизводимости (Data Scientist):**
    *   Код структурирован в репозитории Git.
    *   Присутствует `README.md` с инструкцией по запуску.
    *   Используется `requirements.txt` или `environment.yaml`.
    *   Фиксируются веса модели (в DVC или в Git LFS).
*   **Описание планируемого технического долга (Data Scientist):**
    *   Жесткая логика принятия решения (правила «если-то») — в будущем заменим на классификатор, учитывающий контекст.
    *   Отсутствие обработки видео как последовательности кадров (используется покадровая, а не трекинговая логика).
    *   Модель не оптимизирована (например, с FP16).

### 1.4. Предпосылки решения
*   **Данные:** Используем архивные видеозаписи с камер заказчика (не менее 10 часов, разные погодные условия) + размеченные кадры (bounding boxes тормозного цилиндра, ЗР, вентиля).
*   **Гранулярность модели:** Детекция объектов на уровне отдельных кадров.
*   **Принятие решения:** Агрегация предсказаний по серии кадров, соответствующих одному вагону (консенсус большинства).

---

## 2. Методология (Data Scientist)

### 2.1. Постановка задачи
Техническая задача — **Object Detection (детекция объектов)**. Мы обучаем нейросеть находить на изображении три класса: `brake_cylinder`, `reservoir`, `hand_brake_valve`. Далее, на основе наличия/отсутствия этих объектов в определенных частях кадра (правая/левая половина) и логических правил (для вагонов разных моделей), выносится вердикт об ориентации.

### 2.2. Блок-схема решения (MVP)
*Схема включает бейзлайн (простое правило) и основной MVP (обученная нейросеть).*

1.  **Вход:** Видеопоток / Набор кадров (Frame)
2.  **Этап Pre-processing:**
    *   Изменение размера (Resize)
    *   Нормализация
3.  **Этап Inference (ML):**
    *   Прогон кадра через предобученную CNN (например, YOLOv8, EfficientDet).
    *   *Бейзлайн:* Использование предобученной на COCO модели (без дообучения) для поиска похожих объектов (например, «бутылка» для вентиля).
    *   *MVP:* Дообученная модель на наших размеченных данных.
4.  **Этап Post-processing (Rule Engine):**
    *   Получение списка объектов с координатами.
    *   Определение стороны кадра (левая/правая треть), где найден объект.
    *   Применение правил: *Если найден тормозной цилиндр И он слева -> ориентация "Левая".*
5.  **Этап Агрегации (по вагону):**
    *   Сбор предсказаний для N кадров одного вагона.
    *   Выбор наиболее частой ориентации (мажоритарное голосование).
6.  **Выход:** JSON-сообщение с `wagon_id` (из внешней системы) и `orientation: left/right/unknown`.

### 2.3. Этапы решения задачи

#### **Этап 1: Подготовка данных**

| Название данных | Есть ли данные в компании | Требуемый ресурс | Проверено ли качество данных |
| :--- | :--- | :--- | :--- |
| Сырые видеозаписи (MP4) | Да (предоставит заказчик) | DE/DS (выгрузка) | Нет, нужна проверка на читаемость |
| Размеченные кадры (JSON/COCO) | Нет | DS + Разметчик | Будет проверено в EDA |

*   **Результат этапа:**
    *   Выгрузка 5000+ репрезентативных кадров из видео.
    *   Датасет в формате COCO/YOLO, разбитый на train/val/test (70/15/15).
    *   Отчет EDA: распределение классов по кадрам, анализ сложных условий (снег/ночь).

#### **Этап 2: Разработка Baseline**
*   **Описание:** Проверка гипотезы, что можно определить ориентацию без обучения, используя признаки формы и правила, либо Zero-shot модели (например, Grounding DINO или OWL-ViT).
*   **Формирование выборки:** Тестовая выборка (из Этапа 1).
*   **Целевая метрика:** Accuracy (совпадение предсказанной ориентации с размеченной).
*   **Результат:** Понимание минимального порога качества, от которого будем отталкиваться. Если бейзлайн дает >80%, возможно, ML-дообучение не нужно.

#### **Этап 3: Разработка Core ML (MVP)**
*   **Техника:** Fine-tuning модели YOLOv8m (или v10) на наших данных.
*   **Метрики качества:**
    *   **mAP@0.5:** > 0.90 (для детекции).
    *   **Accuracy ориентации:** > 90% (для бизнес-задачи).
    *   *Связь с бизнесом:* Высокий mAP напрямую влияет на Accuracy правил ориентации.
*   **Кросс-валидация:** Стратифицированная по группам кадров (чтобы кадры одного видео не попали и в трейн, и в тест).
*   **Риски:**
    *   *Риск:* Дисбаланс классов (например, тормозной цилиндр видно всегда, а вентиль — почти никогда).
        *   *План:* Использовать аугментацию данных и взвешивание функции потерь (loss).
    *   *Риск:* Модель путает объекты из-за схожести в условиях снега.
        *   *План:* Сбор дополнительной разметки сложных случаев.
*   **Бизнес-проверка этапа:** Демо Product Owner'у на 3-х минутном видео: показываем, как рисуются bounding boxes и как меняется вердикт "Лево/Право" при проезде вагона.

---

## 3. Подготовка пилота

### 3.1. Способ оценки пилота
Пилот проводится в формате **Shadow Mode (теневой режим)**. Модель подключается параллельно к реальному видеопотоку на одном из объектов заказчика.
*   Результаты модели записываются в лог-файл.
*   Операторы продолжают работать по-старому.
*   В конце недели собранные автоматические метки сравниваются с эталонной выборкой, размеченной экспертом заказчика (blind test).

### 3.2. Что считаем успешным пилотом (Product Owner)
*   **Метрика А (Техническая):** Процент успешно обработанных вагонов (система не упала, выдала результат) > 99%.
*   **Метрика Б (Бизнес):** Accuracy определения стороны на "слепой" выборке заказчика > 90%.
*   **Метрика В (UX):** Время обработки одного вагона не превышает время его проезда (укладываемся в поток).

### 3.3. Подготовка пилота
*   **Вычислительные затраты:** YOLOv8m требует ~ 8-10 GFLOPS на кадр (640x640).
*   **Оценка:** При частоте кадров 5 fps и длине поезда в 50 вагонов, пиковая нагрузка на CPU/GPU будет умеренной.
*   **Решение:** Запрашиваем у заказчика виртуальную машину с 1 GPU (NVIDIA T4 или аналоги) или 8 ядрами CPU (если используем оптимизированный бэкенд OpenVINO/ONNX). Уточним параметры после профилирования бейзлайна.

---

## 4. Внедрение для production систем
*Заполняется, если пилот успешен и требуется внедрение в продуктив.*

### 4.1. Архитектура решения (предлагаемая)
*   **Сервис А (Message Broker):** Kafka/RabbitMQ (источник кадров).
*   **Сервис В (ML Worker):** Наше Python-приложение (FastAPI + Celery).
    1.  Получает кадр.
    2.  Вызывает модель (ONNX Runtime).
    3.  Применяет Rule Engine.
    4.  Публикует результат.
*   **Сервис С (Sink):** Коннектор к БД заказчика (PostgreSQL/ClickHouse).

### 4.2. Описание инфраструктуры и масштабируемости
*   **Выбор:** Контейнеризация (Docker) + Оркестрация (Kubernetes).
*   **Почему:** Позволит легко масштабировать количество ML Worker'ов горизонтально при увеличении количества камер.

### 4.3. Требования к работе системы
*   **SLA:** 99.5% (доступность сервиса).
*   **Задержка (Latency) p95:** < 500 мс от поступления кадра до выдачи результата.

### 4.4. Безопасность системы
*   **Уязвимость:** Отравление данных (Adversarial attacks) через подачу специально искаженного кадра.
*   **Защита:** Валидация входных форматов, ограничение на источники трафика (белые списки IP).

### 4.5. Безопасность данных
*   **GDPR / 152-ФЗ:** Данные не содержат персональных данных (ПДн), так как камера направлена на технические узлы вагона, а не на людей. Тем не менее, необходимо подписать соглашение о неразглашении (NDA) с заказчиком о нераспространении видеозаписей.

### 4.6. Издержки (примерные)
*   **Cloud/On-prem:** Инстанс с 1 x GPU (T4) ~ $300/мес (или амортизация сервера).
*   **Хранение данных:** Логирование результатов (пренебрежимо мало).

### 4.7. Integration points
*   **REST API (sync):** Для управления сервисом (health check, start/stop).
*   **gRPC / Kafka (async):** Основной поток данных (кадры и результаты).

### 4.8. Риски
*   **Риск:** Модель деградирует со временем (изменилась погода, покраска вагонов, замена камер).
    *   *Митигация:* Внедрить систему мониторинга дрейфа данных (data drift) и процесс ручного дообучения (раз в квартал).